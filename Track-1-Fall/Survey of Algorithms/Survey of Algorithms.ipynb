{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSGT: Survey of Models and Algorithms\n",
    "\n",
    "Areas of Focus: \n",
    "- Classification\n",
    "- Regression\n",
    "- Clustering\n",
    "- Dimensionality Reduction  \n",
    "\n",
    "We will practice viewing a problem with a creative angle -- here repurposing housing pricing dataset to make discoveries on crime rates! \n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQGZ-7dAEaqmCg/company-logo_200_200/0?e=2159024400&v=beta&t=-9_7r8w3C8umvoQ8-67w1FcfzHdGQympxHup_2CPof8\" style=\"height:100px\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your lifelong aspiration has been to become a police officer, fighting crime! \n",
    "That said, you keep failing their fitness exams... But you want to help any way you can! \n",
    "You want to show that you can use your coding skills for good instead. You are prototyping a tool that can take in local housing data and determine crime rates in various areas of a city.\n",
    "\n",
    "Using the Boston Housing Price dataset (very overused for explaining price prediciton...) you will find a way to classify high/low crime risk areas. You will also see if you can predict crime rates using regression! \n",
    "\n",
    "<img src=\"./materials/001-cyber.png\" style=\"height:100px\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries needed:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Datasets and preprocessing:\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Neural Network example:\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Exploration  \n",
    "\n",
    "<img src=\"./materials/002-house.png\" style=\"height:100px\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the data\n",
    "boston = load_boston()\n",
    "bostonDF = pd.DataFrame(data = boston.data, columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonDF[\"Price\"] = boston.target\n",
    "# bostonDF.describe()\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quiz: What features do you think are most relevant to crime rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CRIM...... per capita crime rate by town\n",
    "- ZN........       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS.....    proportion of non-retail business acres per town\n",
    "- CHAS......     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX.......      nitric oxides concentration (parts per 10 million)\n",
    "- RM........      average number of rooms per dwelling\n",
    "- AGE.......      proportion of owner-occupied units built prior to 1940\n",
    "- DIS  .......      weighted distances to five Boston employment centres  \n",
    "- RAD  .......      index of accessibility to radial highways  \n",
    "- TAX  .......      full-value property-tax rate per 10,000 \n",
    "- PTRATIO  ...  pupil-teacher ratio by town  \n",
    "- B  .........        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town  \n",
    "- LSTAT  .....    % lower status of the population  \n",
    "- MEDV  ......     Median value of owner-occupied homes in 1000's  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Quiz: How comprehensive is our dataset? \n",
    "Fill in the blanks to find how many missing values we have, and how many datapoints we actually have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_empty(), is_missing(), isnull()\n",
    "missing_val_count = bostonDF.isnull().sum()\n",
    "\n",
    "# .length .width .shape\n",
    "num_instances, num_features = bostonDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example count: \" + str(num_instances) + \"\\n\")\n",
    "print(missing_val_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#####  Quiz: We are looking at crime. But crime in this dataset is the crime in a given town... What is a potential problem with this...? Hint --> what is the code below trying to find? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_value = bostonDF.CRIM.value_counts()\n",
    "print(\"The number of unique values is\", len(important_value)) #get the number of entries in important_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Regression  \n",
    "This dataset is so commonly used for housing price estimation, but let's be different!\n",
    "###### Predicting Crime Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the data\n",
    "non_Crime_df = bostonDF.drop(['CRIM'], axis=1)\n",
    "X = non_Crime_df\n",
    "Y = bostonDF['CRIM']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to measure accuracy and print out score\n",
    "def generate_report(): \n",
    "    # how to get accuracy \n",
    "    y_train_predict = lin_model.predict(X_train)\n",
    "    rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\n",
    "    r2 = r2_score(Y_train, y_train_predict)\n",
    "    \n",
    "    print('\\tTrain', '\\nRMSE is {}'.format(rmse))\n",
    "    print('R2: {}'.format(r2))\n",
    "\n",
    "    # model evaluation for testing set\n",
    "    y_test_predict = lin_model.predict(X_test)\n",
    "    rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "    r2 = r2_score(Y_test, y_test_predict)\n",
    "\n",
    "    print('\\tTest', '\\nRMSE is {}'.format(rmse))\n",
    "    print('R2: {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with all features: Naive! \n",
    "R^2 explains the variance and RMSE looks at distances from prediction to actual values... \n",
    "Given these ranges, are we doing well or poorly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we get better? Lets try fitting a model with fewer features. \n",
    "We will discuss how these were chosen later in the notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_cols = ['INDUS', 'NOX', 'AGE', 'RAD', 'TAX', 'LSTAT', 'Price']\n",
    "X = non_Crime_df[interesting_cols]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=5)\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "\n",
    "generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_cols = [    ]\n",
    "\n",
    "\n",
    "X = non_Crime_df[interesting_cols]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=5)\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "\n",
    "generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quiz: Try some feature combinations and find one that gives an r^2 of under .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try other models: Polynomial Regression and a Decision Tree Regressor\n",
    "Now we will try two other models -- one is a polynomial regressor and the other is a decision tree!\n",
    "\n",
    "\n",
    "<img src=\"./materials/other_reg.png\" style=\"height:250px\"> \n",
    "\n",
    "https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_001.png\n",
    "https://www.nosimpler.me/polynomial-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "interesting_cols = ['INDUS', 'NOX', 'AGE', 'RAD', 'TAX', 'LSTAT', 'Price']\n",
    "X = non_Crime_df[interesting_cols]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "x_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_poly, Y, test_size = 0.3, random_state=5)\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor   \n",
    "\n",
    "interesting_cols = ['INDUS', 'NOX', 'AGE', 'RAD', 'TAX', 'LSTAT', 'Price']\n",
    "X = non_Crime_df[interesting_cols]\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state = 3, max_depth=3, max_features=6, max_leaf_nodes=3)  \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=5)\n",
    "\n",
    "regressor.fit(X_train, Y_train)\n",
    "lin_model = regressor\n",
    "generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Given this output, what are 3 hyperparameters of the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:  Dimensionality Reduction  \n",
    "\n",
    "What happens when we ran regression on ALL of the features?  \n",
    "Our model did not do very well. This phenomenon of hgih dimensionality stems from the data overwhelming the model  \n",
    "Too much data == too much complexity == not enough accuracy   \n",
    "\n",
    "There are several ways to go about solving this issue, including.... \n",
    "- ML algorithms such as PCA or LDA\n",
    "- Forward Feature Selection\n",
    "- Statistical Analysis   \n",
    "- Grid search\n",
    ".... \n",
    "\n",
    "Here we look at statistical analysis and forward feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation and Heat Maps   \n",
    "We can look at correlation coefficients to see which features may be most relevant to crime rates   \n",
    "Do you remember: Which would be a positive, negative, or zero correlation?   \n",
    "\n",
    "\n",
    "<img src=\"./materials/corr.png\" style=\"height:150px\">  \n",
    "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRZmIJtH1sb_HWFhMf71a147tQjeaADhAJ7vQuMpNhmOx5WgmA_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting_cols = ['INDUS', 'NOX', 'AGE', 'RAD', 'TAX', 'LSTAT', 'Price']\n",
    "# sns.pairplot(bostonDF[interesting_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = bostonDF.corr()\n",
    "# sns.heatmap(a, cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = plt.imshow(bostonDF.corr(), cmap = 'PRGn')\n",
    "\n",
    "labels = bostonDF.columns.tolist()\n",
    "\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels, fontsize=12)\n",
    "ax.set_yticklabels(labels, fontsize=12)\n",
    "ax.set_title('Corr Plot')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: From this, what are the most relevant features: Fill in the list and see if you match ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_list_imp_features = [] # ex [\"ZN, \"INDUS\"]\n",
    "our_list_from_before = interesting_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare your model results vs ours\n",
    "# you can see from above what features we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = non_Crime_df[your_list_imp_features]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "x_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_poly, Y, test_size = 0.3, random_state=5)\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = non_Crime_df[interesting_cols]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "x_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_poly, Y, test_size = 0.3, random_state=5)\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feature Selection  \n",
    "Highly intuitive! Iterative method to determine the n most important features  \n",
    "\n",
    "Steps: \n",
    "- For each feature:\n",
    "    - Train your model on that feature and get score\n",
    "    - Keep feature with best performance\n",
    "- For that feature, loop through each other feature\n",
    "    - Train model on that pair\n",
    "    - Keep pair with best performance\n",
    "...\n",
    "- Stop when reach n features or when reach x accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = bostonDF.columns\n",
    "y = bostonDF.iloc[:,0].values #the last column is the band gap\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "for feature in feature_names: \n",
    "    new_X = bostonDF[feature]\n",
    "    new_X = new_X.values.reshape(-1,1)\n",
    "    model = LinearRegression() #create a linear regression model instance\n",
    "    model.fit(new_X, y) #fit the model\n",
    "    r2 = model.score(new_X, y) #get the \"score\", which is equivalent to r^2\n",
    "    print(\"r^2 = {}\".format(r2), feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz:  Which features look the most useful?   \n",
    "\n",
    "We would continue this loop for each pairs and use a threshold to settle at a \"good enough\" solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Dimensionality Reduction with PCA  \n",
    "\n",
    "See the mathematical fundamentals notebook for an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display iris data:\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "x = StandardScaler().fit_transform(x)\n",
    "y = iris.target\n",
    "df = pd.DataFrame(data = iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "components = pca.fit_transform(x)\n",
    "#Show principal components in a table:\n",
    "PC = pd.DataFrame(data = components, columns = ['PC 1', 'PC 2'])\n",
    "PC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('PCA for first and second components', fontsize = 20)\n",
    "\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text(components[y == label, 0].mean(),\n",
    "              components[y == label, 1].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "ax.scatter(components[:,0],\n",
    "            components[:,1],\n",
    "            c = y,\n",
    "            s = 50)\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
