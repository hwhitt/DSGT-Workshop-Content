{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Principals for Data Science\n",
    "\n",
    "Today we go over essential mathematics used in Data Science. This notebook includes example applications for the concepts reviewed in the DSGT lecture presentation. \n",
    "\n",
    "Areas of Focus: \n",
    "- Statistics and Probability \n",
    "- Linear Algebra\n",
    "- Calculus     \n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQGZ-7dAEaqmCg/company-logo_200_200/0?e=2159024400&v=beta&t=-9_7r8w3C8umvoQ8-67w1FcfzHdGQympxHup_2CPof8\" style=\"height:100px\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries needed:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Datasets and preprocessing:\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Neural Network example:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics and Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Boston House Price dataset\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data into a dataframe\n",
    "bostonDF = pd.DataFrame(data = boston.data, columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a price column and adding to dataframe\n",
    "bostonDF[\"Price\"] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        - CRIM     per capita crime rate by town\n",
    "#        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#        - INDUS    proportion of non-retail business acres per town\n",
    "#        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "#        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "#        - RM       average number of rooms per dwelling\n",
    "#        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "#        - DIS      weighted distances to five Boston employment centres\n",
    "#        - RAD      index of accessibility to radial highways\n",
    "#        - TAX      full-value property-tax rate per $10,000\n",
    "#        - PTRATIO  pupil-teacher ratio by town\n",
    "#        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "#        - LSTAT    % lower status of the population\n",
    "#        - MEDV     Median value of owner-occupied homes in $1000's\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the distribution for price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(bostonDF['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and take a look and see which variables correlate to price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(bostonDF['Price'], bostonDF['LSTAT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scatterplot in this cell to show the relationship between Average Rooms and Home Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to run some basic probability calculations. \n",
    "\n",
    "What is the probability that a region has more than 6 rooms per home on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalHomes = len(bostonDF)\n",
    "largeHomes = len(bostonDF[bostonDF[\"RM\"] > 6.0])\n",
    "print(largeHomes/totalHomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about conditional probability? \n",
    "\n",
    "What's the probability that a home has more than 6 rooms given that the price is over 25K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expensiveHomes = bostonDF[bostonDF[\"Price\"] > 25]\n",
    "totalExpensiveHomes = len(expensiveHomes)\n",
    "largeExpensiveHomes = len(expensiveHomes[bostonDF[\"RM\"] > 6.0])\n",
    "print(largeExpensiveHomes / totalExpensiveHomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the probability that a home has more than 6 rooms given that the LSTAT is > 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well doing this is a pain... can we do it faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.heatmap(bostonDF.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra basics with numpy:\n",
    "##### Some examples for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two arrays:\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([4,5,6])\n",
    "\n",
    "# Scalar operations:\n",
    "print(10*x)\n",
    "\n",
    "# Elementwise operations:\n",
    "# Addition:\n",
    "print(x+y)\n",
    "\n",
    "# Subtraction:\n",
    "print(x-y)\n",
    "\n",
    "# Division:\n",
    "print(x/y)\n",
    "\n",
    "# Multiplication (Hadamard product):\n",
    "print(x*y)\n",
    "\n",
    "#Dot Product:\n",
    "print(np.dot(y,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some examples for Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices:\n",
    "a = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6]\n",
    "])\n",
    "\n",
    "b = np.array([[1,2,3]])\n",
    "\n",
    "c = np.array([\n",
    "    [6,5,4],\n",
    "    [3,2,1]\n",
    "])\n",
    "\n",
    "d = np.array([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "# Check sizes:\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(d.shape)\n",
    "\n",
    "# Scalar operations:\n",
    "print(a+1)\n",
    "\n",
    "# Elementwise operations:\n",
    "# Addition:\n",
    "print(a+c)\n",
    "\n",
    "# Subtraction:\n",
    "print(a-c)\n",
    "\n",
    "# Multiplication (Hadamard product):\n",
    "print(a*c)\n",
    "\n",
    "# Matrix multiplication (not elementwise):\n",
    "print(np.dot(d,a))\n",
    "\n",
    "# Transpose:\n",
    "print(d.T)\n",
    "\n",
    "# Inverse:\n",
    "print(np.linalg.inv(d))\n",
    "print(np.dot(np.linalg.inv(d),d)) #Check to see if product is I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display iris data:\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "x = StandardScaler().fit_transform(x)\n",
    "y = iris.target\n",
    "df = pd.DataFrame(data = iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, there are many ways to visualize the iris dataset:\n",
    "<img src=\"Resource/Iris_dataset_scatterplot.png\" width=\"500\">\n",
    "\n",
    "PCA can help simplify things by reducing the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "components = pca.fit_transform(x)\n",
    "#Show principal components in a table:\n",
    "PC = pd.DataFrame(data = components, columns = ['PC 1', 'PC 2'])\n",
    "PC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('PCA for first and second components', fontsize = 20)\n",
    "\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text(components[y == label, 0].mean(),\n",
    "              components[y == label, 1].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "ax.scatter(components[:,0],\n",
    "            components[:,1],\n",
    "            c = y,\n",
    "            s = 50)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are computational models where many simple units work in parallel and updating the weights between these units helps the network learn new information.\n",
    "\n",
    "Components of a neural network:\n",
    "- Neurons\n",
    "- Layers\n",
    "- Connections\n",
    "\n",
    "<img src=\"Resource/nn_Patterson_Gibson_DeepLearning.png\" width=\"500\">\n",
    "\n",
    "An equation to model this this is: X*w + b = y, where X is a feature matrix, w is the vector of weights, b is a bias and y is an output.\n",
    "\n",
    "A perceptron is a linear-model binary classifier with a simple input/output relationship. A number of inputs is given, they are summed after being given their associated weights, and an output is determined.\n",
    "Similarly, a multilayer perceptron is a network with one or more hidden layers.\n",
    "\n",
    "Below, we use sklearn's multi-layer perceptron classifier to attempt to classify a species of iris with given petal/sepal measurements based on our iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and testing sets:\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "#Train a Multi-Layer Perceptron Classifier (Neural Network):\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points of interest:\n",
    "- Notice the solver used is 'adam'. This is a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba. What if we changed to just using stochastic gradient decent ('sgd')?\n",
    "- There is one hidden layer, what happens when you change the size? How about adding more layers?\n",
    "- How does changing the type of solver for weight optimization effect the number of iterations needed to converge (you might need to turn on warnings to see what happens)?\n",
    "\n",
    "### Evaluating our model:\n",
    "Below are evaluations of our test results.\n",
    "\n",
    "An explanation of each measurement (Let TP, TN, FP, and FN correspond to True/False Negatives/Positives):\n",
    "\n",
    "Accuracy: The degree of closeness of the predicted to the true value\n",
    "- Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "Precision: The degree to which repeated predictions under the same conditions give the same results\n",
    "- Precision = TP/(TP+FP)\n",
    "\n",
    "Recall (TP rate, Sensitivity): How well the model avoids false negatives\n",
    "- Recall = TP/(TP+FN)\n",
    "\n",
    "F1: measure of the model's accuracy using precision and recall measures\n",
    "- F1 = 2TP/(2TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(x_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sb.heatmap(cm, annot=True, fmt=\".0f\", linewidth=0.5, square=True, cmap=\"Blues_r\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.title(\"Accuracy Score: {0}%\".format(round(accuracy * 100, 2)), size=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
