{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy: Numeric Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Resource/ndarray.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fundamental Object: ndarray\n",
    "\n",
    "- Vectorized operations on arrays\n",
    "\n",
    "- Broadcasting\n",
    "\n",
    "- File IO and memory-mapped files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1D Array\n",
    "  - 1d_array = np.array([1,2,3])\n",
    "\n",
    "- 2D Array\n",
    "  - 2d_array = np.array([[1,2,3], [4,5,6]])\n",
    "  \n",
    "- 3D Array\n",
    "  - 3d_array = np.array([[[1,2,3], [4,5,6]], [[11,12,13], [14,15,16]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Resource/series-and-dataframe.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Series: 1D labeled array capable of holding any data type with axis labels or index\n",
    "\n",
    "- DataFrame: two-dimensional labeled data structures with columns of potentially different types, including:\n",
    "  - DataFrame\n",
    "  - Series\n",
    "  - Numpy ndarray\n",
    "  - Dictionaries of ndarrays, lists, dictionaries or Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2D plotting library which produces publication quality figures\n",
    "\n",
    "  - Line plots, scatter plots, histograms, pie charts, etc.\n",
    "\n",
    "\n",
    "- Useable in Python scripts, the Python and IPython shells, the Jupyter Notebook, etc.\n",
    "\n",
    "  - Not built upon Numpy/Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file using pandas\n",
    "data = pd.read_csv('nyc-rolling-sales.csv')\n",
    "\n",
    "# Peak into the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of useless data by inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'UnNamed: 0' column because it seems just to be number of iterations\n",
    "# Drop 'EASE-MENT' column because it is empty\n",
    "data = data.drop(['Unnamed: 0', 'EASE-MENT', 'SALE DATE'], axis = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any duplicated entry\n",
    "data.duplicated(data.columns).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated entries and keep the last occurence of each\n",
    "data = data.drop_duplicates(data.columns, keep = 'last')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at all the columns/features in the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver the columns to its proper data type\n",
    "data['BUILDING CLASS CATEGORY'] = data['BUILDING CLASS CATEGORY'].astype('category')\n",
    "data['TAX CLASS AT TIME OF SALE'] = data['TAX CLASS AT TIME OF SALE'].astype('category')\n",
    "data['TAX CLASS AT PRESENT'] = data['TAX CLASS AT PRESENT'].astype('category')\n",
    "data['LAND SQUARE FEET'] = pd.to_numeric(data['LAND SQUARE FEET'], errors='coerce')\n",
    "data['GROSS SQUARE FEET']= pd.to_numeric(data['GROSS SQUARE FEET'], errors='coerce')\n",
    "data['SALE PRICE'] = pd.to_numeric(data['SALE PRICE'], errors='coerce')\n",
    "data['BOROUGH'] = data['BOROUGH'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns contain null data\n",
    "data.columns[data.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the proportion of null data in the columns\n",
    "col_with_missing_data = data.isnull().sum() / len(data)\n",
    "col_with_missing_data = col_with_missing_data[col_with_missing_data > 0]\n",
    "col_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null data with the median of the column\n",
    "data['LAND SQUARE FEET'] = data['LAND SQUARE FEET'].fillna(data['LAND SQUARE FEET'].median())\n",
    "data['GROSS SQUARE FEET'] = data['GROSS SQUARE FEET'].fillna(data['GROSS SQUARE FEET'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at our sale prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove outlier sale prices from our data\n",
    "# price_q1 = data['SALE PRICE'].quantile(0.25)\n",
    "# price_q3 = data['SALE PRICE'].quantile(0.75)\n",
    "# price_IQR = price_q3 - price_q1\n",
    "# upper = price_q3 + 1.5 * price_IQR\n",
    "# lower = price_q1 - 1.5 * price_IQR\n",
    "# data = data[(data['SALE PRICE'] < upper) & (data['SALE PRICE'] > lower)]\n",
    "\n",
    "# Plot a simple histogram of the sale prices\n",
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "ax.hist(data['SALE PRICE'], bins = 40)\n",
    "ax.set_xlabel('Price')\n",
    "ax.set_ylabel('Number of Sales')\n",
    "ax.set_title('Sale Price Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any outlier or useless data in the TOTAL UNIT column\n",
    "data[[\"TOTAL UNITS\", \"SALE PRICE\"]].groupby(['TOTAL UNITS'], as_index=False).count().sort_values(by='SALE PRICE', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the entry with 0 TOTAL UNIT and the entry with outlier 2261 TOTAL UNIT\n",
    "data = data[(data['TOTAL UNITS'] > 0) & (data['TOTAL UNITS'] != 2261)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of more columns that might not be a factor of sale price\n",
    "data = data.drop(['ADDRESS', 'NEIGHBORHOOD', 'BUILDING CLASS AT PRESENT', 'BUILDING CLASS AT TIME OF SALE', 'APARTMENT NUMBER'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of numeric data\n",
    "numeric_data = data.select_dtypes(include = [np.number])\n",
    "numeric_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(data[numeric_data.columns])\n",
    "scaled = scaler.transform(data[numeric_data.columns])\n",
    "for i, col in enumerate(numeric_data.columns):\n",
    "       data[col] = scaled[:, i]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = data.select_dtypes(exclude = [np.number])\n",
    "categorical_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables into dummy/indicator variables (i.e. one-hot encoding).\n",
    "one_hot_encoded = pd.get_dummies(data[categorical_data.columns])\n",
    "one_hot_encoded.info(verbose = True, memory_usage = True, null_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original categorical columns and replace them with their corresponding one-hot encoded columns\n",
    "df = data.drop(categorical_data.columns, axis = 1)\n",
    "df = pd.concat([df, one_hot_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at what we have so far\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with our data to predict our sale prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features for training and labels\n",
    "y_df = df['SALE PRICE']\n",
    "x_df = df.drop('SALE PRICE', axis = 1)\n",
    "\n",
    "# What do the shapes tell us?\n",
    "x_df.shape , y_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "X_train ,X_test, Y_train , Y_test = train_test_split(x_df , y_df , test_size = 0.3 , random_state =34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our training data to a linear regression model and predict\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, Y_train)\n",
    "Y_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how well our model performs\n",
    "mean_squared_error(Y_test, Y_pred, multioutput='raw_values')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
