{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <INSERT WORKSHOP TITLE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Natural Language Processing with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Visualizing Spacy's Architecture\n",
    "\n",
    "![alt](https://d33wubrfki0l68.cloudfront.net/27b15441d6fb921213fcca5addc324d3b00a9d15/2c15d/architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg)\n",
    "\n",
    "For this workshop, these are the only points we need to take away\n",
    "\n",
    "- The **Document** is the big encompassing container\n",
    "- The **Token** is exactly what you think it is (mostly a word)\n",
    "- The **Span** is simply a sequence of tokens that is sliced from a Document\n",
    "- The Document is produced by the **Tokenizer** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required packages\n",
    "\n",
    "`pip3 install -U spacy`  \n",
    "`python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp('Hello    World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic word tokenization\n",
    "\n",
    "Our document variable is simply a sequence of tokens. We can thus iterate through it and print out the tokens parsed. Spacy goes one step further and performs index-preserving word tokenization, i.e., every token (including spaces) retains its place in the sentence. Other libraries like NLTK do not offer this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the tokens parsed by the document\n",
    "for token in document:\n",
    "    print(\"\\\"%d: %s\\\"\" % (token.idx, token.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the attributes of the token class\n",
    "\n",
    "Let us have a look at the word-level attributes of a token class. They reveal a wealth of information about the words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(\"Next week I'll   be in Madrid.\")\n",
    "print(\"WORD\\tINDEX\\tLEMMA\\tPUNC\\tSPACE\\tSHAPE\\tPOSITION\\tTAG\")\n",
    "for token in document:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploring Spacy's Toolbox\n",
    "\n",
    "Remember the parts of the example NLP pipeline we looked at? Let's see the features that Spacy offers us to streamline that process!\n",
    "\n",
    "The list of processes we will look at are:\n",
    "\n",
    "1. Sentence segmentation\n",
    "2. Parts of speech tagging ([View the codes](https://spacy.io/usage/linguistic-features))\n",
    "3. Named Entity Recognition ([View Entity Codes](https://spacy.io/usage/linguistic-features#entity-types))\n",
    "4. Dependency Parsing\n",
    "5. Visualizations using Displacy\n",
    "6. Similarity based on word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence segmentation\n",
    "document = nlp(\"Three French Hens.    Two turtle doves.     And a Partridge in a pear tree.\")\n",
    "\n",
    "for sentence in document.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of speech tagging\n",
    "document = nlp(\"Next week I'll be in Madrid. \")\n",
    "\n",
    "print([(token.text, token.tag_) for token in document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition\n",
    "document = nlp(\"Madrid is the capital of Spain. It is home to the sporting giant Real Madrid.\")\n",
    "\n",
    "for entity in document.ents:\n",
    "    print(\"%s: %s\" % (entity.text, entity.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the variety of entities.\n",
    "document = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
    "print(\"ENTITY\\tLABEL\")\n",
    "for ent in document.ents:\n",
    "    print(\"{0}\\t{1}\".format(\n",
    "        ent.text, \n",
    "        ent.label_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing entity recognition using displacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(document, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun phrases and chunking\n",
    "document = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "print(\"TEXT\\t\\t\\tLABEL\\tROOT\")\n",
    "for chunk in document.noun_chunks:\n",
    "    print(\"%s\\t%s\\t%s\" % (chunk.text, chunk.label_, chunk.root.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Parsing\n",
    "document = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "\n",
    "for token in document:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, \n",
    "        token.tag_, \n",
    "        token.dep_, \n",
    "        token.head.text, \n",
    "        token.head.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the dependency parsing\n",
    "displacy.render(document, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors and Similarity\n",
    "\n",
    "We will not explore the concept behind a Word vector in this workshop. \n",
    "\n",
    "Explaining word vectors(aka word embeddings) are not the purpose of this tutorial. Here are a few properties word vectors have:\n",
    "\n",
    "- If two words are similar, they appear in similar contexts\n",
    "- Word vectors are computed taking into account the context (surrounding words)\n",
    "- Given the two previous observations, similar words should have similar word vectors\n",
    "- Using vectors we can derive relationships between words\n",
    "\n",
    "Additionally, we will need a larger english model for this. Download it using the following command:\n",
    "\n",
    "`python -m spacy download en_core_web_lg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "mango = nlp('mango')\n",
    "print(\"Shape: %d\" % mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing similarity\n",
    "banana = nlp('banana')\n",
    "dog = nlp('dog')\n",
    "fruit = nlp('fruit')\n",
    "animal = nlp('animal')\n",
    " \n",
    "print(dog.similarity(animal), dog.similarity(fruit))\n",
    "print(banana.similarity(fruit), banana.similarity(animal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: Language Processing Pipelines\n",
    "\n",
    "Fundamentally, a pipeline is a list of functions called on a Doc in order. The pipeline can be set by a model, and modified by the user. A pipeline component can be a complex class that holds state, or a very simple Python function that adds something to a Doc and returns it.\n",
    "\n",
    "![alt](https://user-images.githubusercontent.com/13643239/55229632-dbff9480-521d-11e9-8499-efb2a9c948db.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(\"NLP Pipe: \", nlp.pipe_names)\n",
    "print(\"Pipeline: \\n\", nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pipeline Components\n",
    "\n",
    "Sometimes, we'd want to modify the pipeline by adding in our own custom components to perform specific tasks. Spacy offers this functionality in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_component(doc):\n",
    "    print(\"The doc is {} characters long and has {} tokens.\".format(len(doc.text), len(doc)))\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component, name='print_length', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources and Additional Readings\n",
    "[Spacy Documentation](https://spacy.io/api/) .   \n",
    "[A Formal Introduction to NLP](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) .   \n",
    "[Solving NLP Problems using Pipelines](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)\n",
    "[Classifying Text using Spacy](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/)  \n",
    "[Introducing Custom Components in Spacy](https://explosion.ai/blog/spacy-v2-pipelines-extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
