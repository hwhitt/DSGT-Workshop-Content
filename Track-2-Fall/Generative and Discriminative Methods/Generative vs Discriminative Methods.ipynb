{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative vs Discriminative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at generative and discriminative approaches towards classification problems and also explore an application of generative methods in a generative adversarial network (GAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Data:\n",
    "<img src=\"Resources/wine.jpg\" width=\"250\">\n",
    "Making wine is pretty interesting, where many different factors play a role in determining the properties of the wine. This dataset presents a chemical and physical analysis of wine from 3 different sources in Italy.<sup>1</sup>\n",
    "\n",
    "<sup>1. Forina, M. et al, PARVUS - An Extendible Package for Data\n",
    "       Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
    "       and Food Analysis and Technologies, Via Brigata Salerno, \n",
    "       16147 Genoa, Italy.</sup>\n",
    "\n",
    "#### Brief description of features:\n",
    "1. Cultivar: source of wine\n",
    "2. Alcohol: alcohol content\n",
    "3. Malic acid (C4H6O5): Found in fruits, contributes sour taste\n",
    "4. Ash: inorganic matter\n",
    "5. Alkalinity of ash: how basic the ash is\n",
    "6. Magnesium: magnesium content, a cofactor in many enzyme systems that regulate biochemical reactions in the body\n",
    "7. Total phenols: natural compounds containing phenol group that contribute to the color and texture in wine\n",
    "8. Flavanoids: a type of phenol, most of the phenols in wine are flavanoids\n",
    "9. Nonflavanoid phenols: all the other phenols\n",
    "10. Proanthocyanidins: polyphenols, composed of flavanoid oligomers\n",
    "11. Color intensity: measurement made with spectrophotometer/colorometer to determine transmission properties of the wine\n",
    "12. Hue: a property of color of the wine\n",
    "13. OD280/OD315 of diluted wines: optical density at 280nm/315nm ratio, like absorbance except it considers the scattering of light as well. Used to determine protein concentration\n",
    "14. Proline(C5H9NO2): The most abundant amino acid in wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wine.data',names = ['Cultivar','Alcohol','Malic_acid','Ash','Alkalinity_of_ash','Magnesium',\n",
    "                                 'Total_phenols','Flavanoids','Nonflavanoid_phenols','Proanthocyanidins','Color_intensity',\n",
    "                                 'Hue','OD280/OD315_of_diluted_wines','Proline'], index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are an application of Bayes theorem with naive independence assumptions between the features. \n",
    "<img src=\"Resources/Bayes.png\" width=\"300\">\n",
    "P(A|B): Posterior\n",
    "\n",
    "P(A): Prior\n",
    "\n",
    "P(B|A): Likelihood\n",
    "\n",
    "P(B): Evidence\n",
    "\n",
    "Given that A is a class we are trying to predict using B as an independent variable, we notice that the denominator in Bayes theorem is constant as P(B) when the features are known. It follows that the conditional probability P(A|B) can be found using the joint probability model. Notice that P(A|B) is used to determine decision boundaries, just as in discriminative methods, but it is calculated using estimates of P(B|A) and the probability distribution of the data is learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the probability distrubution of each class of wine from our dataset using the distribution for flavanoid content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution for Cultivar 1, 2, and 3\n",
    "x = df[['__________']].drop([2,3])\n",
    "sb.distplot(x, label = 'Cultivar 1')\n",
    "x = df[['__________']].drop([1,3])\n",
    "sb.distplot(x, label = 'Cultivar 2')\n",
    "x = df[['__________']].drop([1,2])\n",
    "sb.distplot(x, label = 'Cultivar 3')\n",
    "\n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12})\n",
    "plt.title('Flavanoid content of 3 Cultivar Sources')\n",
    "plt.xlabel('Flavanoids')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that each class has a normal distribution, how would we determine the joint probability using the this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we will most likely have more than one feature to look at. Let's use two features of flavanoids and alcohol content and try to create a classification model using naive Bayes!. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of our data with each point already labeled\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_axes([.1,.1,.8,.8])\n",
    "ax.scatter(df.Flavanoids, df['Alcohol'], c=df.index, edgecolors='k', cmap=plt.cm.Paired)\n",
    "ax.set_xlabel('Flavanoids')\n",
    "ax.set_ylabel('Alcohol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training sets:\n",
    "X = df[['Flavanoids','Alcohol']]\n",
    "Y = df.index\n",
    "__________,__________,__________,__________ = train_test_split(X,Y)\n",
    "\n",
    "# Fit the data:\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Results:\n",
    "training_score = clf.score(X_train, Y_train)\n",
    "print(\"training score: \",training_score)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "print(\"test score:     \", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Despite its simple design, naive Bayes performs quite well given that its assumptions hold. Notice that we use Gaussian naive Bayes method from scikit, which assumes a continuous normal distribution. P(x|y) is estimated using the following equation:\n",
    "<img src=\"Resources/gaussianBayes.png\" width=\"300\">\n",
    "where C_k is a class, x is a feature, and sigma^2 and mu are variance and mean respectively.\n",
    "\n",
    "The probability distribution is visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "# Plot the decision boundary in a mesh:\n",
    "x_min, x_max = X['Flavanoids'].min() - .5, X['Flavanoids'].max() + .5\n",
    "y_min, y_max = X['Alcohol'].min() - .5, X['Alcohol'].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=.8)\n",
    "plt.figure(1, figsize=(8, 8))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Blues)\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Oranges, alpha=.5)\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,2]\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Greys, alpha=.3)\n",
    "\n",
    "# Plot the points:\n",
    "plt.scatter(df.Flavanoids, df['Alcohol'], c=df.index, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Flavanoids')\n",
    "plt.ylabel('OD280/OD315_of_diluted_wines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regressions directly estimate P(y|x) using the logistic equation:\n",
    "<img src=\"Resources/logistic.png\" width=\"300\">\n",
    "Because it is a discriminative method, it does not look at the joint probability. We will use logistic regression this time to classify our wine with the same flavanoid and alcohol features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training sets:\n",
    "X = df[['Flavanoids','Alcohol']]\n",
    "Y = df.index\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)\n",
    "\n",
    "# Fit the data:\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')   #Notice the solver settings\n",
    "logreg.fit(__________, __________)\n",
    "\n",
    "# Results:\n",
    "training_score = logreg.score(__________, __________)\n",
    "print(\"training score: \",training_score)\n",
    "test_score = logreg.score(__________, __________)\n",
    "print(\"test score:     \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "# Plot the decision boundary in a mesh:\n",
    "x_min, x_max = X['Flavanoids'].min() - .5, X['Flavanoids'].max() + .5\n",
    "y_min, y_max = X['Alcohol'].min() - .5, X['Alcohol'].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=.8)\n",
    "plt.figure(1, figsize=(8, 8))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the points:\n",
    "plt.scatter(df.Flavanoids, df['Alcohol'], c=df.index, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Flavanoids')\n",
    "plt.ylabel('Alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in decision boundary compared to naive Bayes. How does logistic regression as a discriminative method compare to naive Bayes as a generative method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GAN consists of to models that work together to synthesize data. A generator model is fed input noise to produce a fake image, while a discriminator model takes in this fake along with a real image and outputs whether it thinks the image is fake or real. By simultaneously training the generator to better generate fake images and the discriminator to better distinguish real images, the generator will eventually produce a sample that can fool the discriminator into thinking it is real.\n",
    "<img src=\"Resources/gan.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method requires TensorFlow and Keras, which are used for machine learning neural networks!\n",
    "\n",
    "They can be installed with:\n",
    "- pip install tensorflow OR conda install tensorflow\n",
    "- pip install keras OR conda install keras\n",
    "\n",
    "The following code was taken from https://github.com/MonteChristo46/GAN-Notebooks/blob/master/GAN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 60000\n",
    "EPOCHES = 300\n",
    "OUTPUT_DIR = \"img\" # The output directory where the images of the generator a stored during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.shape)\n",
    "plt.imshow(train_images[1], cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype(\"float32\")\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images.reshape(train_images.shape[0],784)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "\n",
    "    def __init__(self, random_noise_size = 100):\n",
    "        super().__init__(name='generator')\n",
    "        #layers\n",
    "        self.input_layer = keras.layers.Dense(units = random_noise_size)\n",
    "        self.dense_1 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_1 =  keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_2 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_2 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_3 = keras.layers.Dense(units = 256)\n",
    "        self.leaky_3 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.output_layer = keras.layers.Dense(units=784, activation = \"tanh\")\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        ## Definition of Forward Pass\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.leaky_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.leaky_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.leaky_3(x)\n",
    "        return  self.output_layer(x)\n",
    "    \n",
    "    def generate_noise(self,batch_size, random_noise_size):\n",
    "        return np.random.uniform(-1,1, size = (batch_size, random_noise_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "\n",
    "def generator_objective(dx_of_gx):\n",
    "    # Labels are true here because generator thinks he produces real images. \n",
    "    return cross_entropy(tf.ones_like(dx_of_gx), dx_of_gx) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "fake_image = generator(np.random.uniform(-1,1, size =(1,100)))\n",
    "fake_image = tf.reshape(fake_image, shape = (28,28))\n",
    "plt.imshow(fake_image, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name = \"discriminator\")\n",
    "        \n",
    "        #Layers\n",
    "        self.input_layer = keras.layers.Dense(units = 784)\n",
    "        self.dense_1 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_1 =  keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_2 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_2 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_3 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_3 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        \n",
    "        self.logits = keras.layers.Dense(units = 1)  # This neuron tells us if the input is fake or real\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "          ## Definition of Forward Pass\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.leaky_1(x)\n",
    "        x = self.leaky_2(x)\n",
    "        x = self.leaky_3(x)\n",
    "        x = self.leaky_3(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_objective(d_x, g_z, smoothing_factor = 0.9):\n",
    "    \"\"\"\n",
    "    d_x = real output\n",
    "    g_z = fake output\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(tf.ones_like(d_x) * smoothing_factor, d_x) # If we feed the discriminator with real images, we assume they all are the right pictures --> Because of that label == 1\n",
    "    fake_loss = cross_entropy(tf.zeros_like(g_z), g_z) # Each noise we feed in are fakes image --> Because of that labels are 0. \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = keras.optimizers.RMSprop()\n",
    "discriminator_optimizer = keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def training_step(generator: Discriminator, discriminator: Discriminator, images:np.ndarray , k:int =1, batch_size = 32):\n",
    "    for _ in range(k):\n",
    "         with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            noise = generator.generate_noise(batch_size, 100)\n",
    "            g_z = generator(noise)\n",
    "            d_x_true = discriminator(images) # Trainable?\n",
    "            d_x_fake = discriminator(g_z) # dx_of_gx\n",
    "\n",
    "            discriminator_loss = discriminator_objective(d_x_true, d_x_fake)\n",
    "            # Adjusting Gradient of Discriminator\n",
    "            gradients_of_discriminator = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) # Takes a list of gradient and variables pairs\n",
    "            \n",
    "              \n",
    "            generator_loss = generator_objective(d_x_fake)\n",
    "            # Adjusting Gradient of Generator\n",
    "            gradients_of_generator = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.uniform(-1,1, size = (1, 100)) # generating some noise for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make sure the output directory exists..\n",
    "import os\n",
    "if not os.path.exists(\"img\"):\n",
    "    os.makedirs(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(dataset, epoches):\n",
    "    for epoch in range(epoches):\n",
    "        for batch in dataset: \n",
    "            training_step(generator, discriminator, batch ,batch_size = BATCH_SIZE, k = 1)\n",
    "            \n",
    "        ## After ith epoch plot image \n",
    "        if (epoch % 50) == 0: \n",
    "            fake_image = tf.reshape(generator(seed), shape = (28,28))\n",
    "            print(\"{}/{} epoches\".format(epoch, epoches))\n",
    "            #plt.imshow(fake_image, cmap = \"gray\")\n",
    "            plt.imsave(\"{}/{}.png\".format(OUTPUT_DIR,epoch),fake_image, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(train_dataset, EPOCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image = generator(np.random.uniform(-1,1, size = (1, 100)))\n",
    "plt.imshow(tf.reshape(fake_image, shape = (28,28)), cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
